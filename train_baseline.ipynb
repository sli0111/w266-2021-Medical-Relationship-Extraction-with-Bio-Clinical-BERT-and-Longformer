{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "viral-factory",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "separated-concentrate",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers\n",
    "!pip install tdqm\n",
    "#!pip install tensorflow\n",
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "referenced-longer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import torch\n",
    "# print(torch.__version__)\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-madness",
   "metadata": {},
   "source": [
    "# Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "massive-connection",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"output_attentions\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.8.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 28996\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer and Models\n",
    "# from transformers import LongformerConfig, LongformerTokenizerFast, TFLongformerModel\n",
    "\n",
    "# tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096')\n",
    "# transformer_model = TFLongformerModel.from_pretrained('allenai/longformer-base-4096',\n",
    "#                                                       output_attentions=True)\n",
    "# transformer_model.config\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "transformer_model = TFAutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", from_pt=True,\n",
    "                                               output_attentions=True)\n",
    "transformer_model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-district",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "neural-train",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>TLINK</th>\n",
       "      <th>Target_A</th>\n",
       "      <th>Target_B</th>\n",
       "      <th>Text</th>\n",
       "      <th>End</th>\n",
       "      <th>Words</th>\n",
       "      <th>left_0</th>\n",
       "      <th>left_1</th>\n",
       "      <th>right_0</th>\n",
       "      <th>right_1</th>\n",
       "      <th>distance</th>\n",
       "      <th>Target_Text_Pair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>612_SECTIME1</td>\n",
       "      <td>AFTER</td>\n",
       "      <td>further evaluation</td>\n",
       "      <td>2014-08-07</td>\n",
       "      <td>Admission Date : [R] 2014-08-07 [R] Discharge ...</td>\n",
       "      <td>end</td>\n",
       "      <td>['Admission' 'Date' ':' '[R]' '2014-08-07' '[R...</td>\n",
       "      <td>188</td>\n",
       "      <td>191</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>188</td>\n",
       "      <td>further evaluation [SEP] 2014-08-07 [SEP] Admi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>123_TL12</td>\n",
       "      <td>OVERLAP</td>\n",
       "      <td>NPO</td>\n",
       "      <td>IVF</td>\n",
       "      <td>Admission Date : 2013-11-21 Discharge Date : 2...</td>\n",
       "      <td>end</td>\n",
       "      <td>['Admission' 'Date' ':' '2013-11-21' 'Discharg...</td>\n",
       "      <td>65</td>\n",
       "      <td>67</td>\n",
       "      <td>69</td>\n",
       "      <td>71</td>\n",
       "      <td>2</td>\n",
       "      <td>NPO [SEP] IVF [SEP] Admission Date : 2013-11-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>413_TL22</td>\n",
       "      <td>AFTER</td>\n",
       "      <td>anti-Candida regimen</td>\n",
       "      <td>consultation</td>\n",
       "      <td>ADMISSION DATE : 5-3-93 DISCHARGE DATE : 5-12-...</td>\n",
       "      <td>end</td>\n",
       "      <td>['ADMISSION' 'DATE' ':' '5-3-93' 'DISCHARGE' '...</td>\n",
       "      <td>173</td>\n",
       "      <td>176</td>\n",
       "      <td>81</td>\n",
       "      <td>83</td>\n",
       "      <td>95</td>\n",
       "      <td>anti-Candida regimen [SEP] consultation [SEP] ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>492_TL0</td>\n",
       "      <td>AFTER</td>\n",
       "      <td>elective coronary artery bypass grafting</td>\n",
       "      <td>Admission</td>\n",
       "      <td>[R] Admission [R] Date : 2016-02-14 Discharge ...</td>\n",
       "      <td>end</td>\n",
       "      <td>['[R]' 'Admission' '[R]' 'Date' ':' '2016-02-1...</td>\n",
       "      <td>91</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>97</td>\n",
       "      <td>elective coronary artery bypass grafting [SEP]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>337_TL19</td>\n",
       "      <td>AFTER</td>\n",
       "      <td>monitoring</td>\n",
       "      <td>referred</td>\n",
       "      <td>Admission Date : 2017-06-12 Discharge Date : 2...</td>\n",
       "      <td>end</td>\n",
       "      <td>['Admission' 'Date' ':' '2017-06-12' 'Discharg...</td>\n",
       "      <td>79</td>\n",
       "      <td>81</td>\n",
       "      <td>74</td>\n",
       "      <td>76</td>\n",
       "      <td>7</td>\n",
       "      <td>monitoring [SEP] referred [SEP] Admission Date...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            ID    TLINK  \\\n",
       "0           0  612_SECTIME1    AFTER   \n",
       "1           1      123_TL12  OVERLAP   \n",
       "2           2      413_TL22    AFTER   \n",
       "3           3       492_TL0    AFTER   \n",
       "4           4      337_TL19    AFTER   \n",
       "\n",
       "                                   Target_A      Target_B  \\\n",
       "0                        further evaluation    2014-08-07   \n",
       "1                                       NPO           IVF   \n",
       "2                      anti-Candida regimen  consultation   \n",
       "3  elective coronary artery bypass grafting     Admission   \n",
       "4                                monitoring      referred   \n",
       "\n",
       "                                                Text  End  \\\n",
       "0  Admission Date : [R] 2014-08-07 [R] Discharge ...  end   \n",
       "1  Admission Date : 2013-11-21 Discharge Date : 2...  end   \n",
       "2  ADMISSION DATE : 5-3-93 DISCHARGE DATE : 5-12-...  end   \n",
       "3  [R] Admission [R] Date : 2016-02-14 Discharge ...  end   \n",
       "4  Admission Date : 2017-06-12 Discharge Date : 2...  end   \n",
       "\n",
       "                                               Words  left_0  left_1  right_0  \\\n",
       "0  ['Admission' 'Date' ':' '[R]' '2014-08-07' '[R...     188     191        3   \n",
       "1  ['Admission' 'Date' ':' '2013-11-21' 'Discharg...      65      67       69   \n",
       "2  ['ADMISSION' 'DATE' ':' '5-3-93' 'DISCHARGE' '...     173     176       81   \n",
       "3  ['[R]' 'Admission' '[R]' 'Date' ':' '2016-02-1...      91      97        0   \n",
       "4  ['Admission' 'Date' ':' '2017-06-12' 'Discharg...      79      81       74   \n",
       "\n",
       "   right_1  distance                                   Target_Text_Pair  \n",
       "0        5       188  further evaluation [SEP] 2014-08-07 [SEP] Admi...  \n",
       "1       71         2  NPO [SEP] IVF [SEP] Admission Date : 2013-11-2...  \n",
       "2       83        95  anti-Candida regimen [SEP] consultation [SEP] ...  \n",
       "3        2        97  elective coronary artery bypass grafting [SEP]...  \n",
       "4       76         7  monitoring [SEP] referred [SEP] Admission Date...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resample shape:  (9000, 14)\n",
      "test shape:  (11200, 14)\n"
     ]
    }
   ],
   "source": [
    "# train_path = '/home/ubuntu/Longformer/train_test_data/mini_df.csv'\n",
    "test_path = '/home/ubuntu/Longformer/train_test_data/mini_test_df.csv'\n",
    "resample_path = '/home/ubuntu/Longformer/train_test_data/resample_df.csv'\n",
    "\n",
    "# Get all the data\n",
    "test_df = pd.read_csv(test_path)\n",
    "resample_df = pd.read_csv(resample_path)\n",
    "\n",
    "# Add separator\n",
    "resample_df['Target_Text_Pair'] = list(map(lambda x,y,z: x + \" \" + tokenizer.sep_token + \" \" + y + \" \" + tokenizer.sep_token + \" \" + z, \n",
    "                                        resample_df['Target_A'], resample_df['Target_B'], resample_df['Text']))\n",
    "\n",
    "test_df['Target_Text_Pair'] = list(map(lambda x,y,z: x + \" \" + tokenizer.sep_token + \" \" + y + \" \" + tokenizer.sep_token + \" \" + z, \n",
    "                                        test_df['TargetA'], test_df['Target_B'], test_df['Text']))\n",
    "\n",
    "display(resample_df.head())\n",
    "\n",
    "print('resample shape: ', resample_df.shape)\n",
    "print('test shape: ', test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aerial-dealer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "[0.33333333 0.33333333 0.33333333]\n",
      "\n",
      "test\n",
      "[0.10017857 0.36410714 0.53571429]\n"
     ]
    }
   ],
   "source": [
    "# Map labels to numbers\n",
    "TLINK_map = {'AFTER':0,\n",
    "             'OVERLAP': 1,\n",
    "             'BEFORE': 2}\n",
    "\n",
    "y_train = np.asarray(resample_df['TLINK'].apply(lambda x: TLINK_map[x])).reshape(-1,1)\n",
    "y_test = np.asarray(test_df['TLINK'].apply(lambda x: TLINK_map[x])).reshape(-1,1)\n",
    "\n",
    "# Distribution\n",
    "print('train')\n",
    "dist = np.unique(y_train, return_counts=True)[1]/sum(np.unique(y_train, return_counts=True)[1])\n",
    "print(dist)\n",
    "\n",
    "print()\n",
    "print('test')\n",
    "dist = np.unique(y_test, return_counts=True)[1]/sum(np.unique(y_test, return_counts=True)[1])\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-jumping",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "violent-cookie",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(max_length, sentences, tokenizer, \n",
    "             left_token=None, right_token=None, entity_mask=False, \n",
    "             global_attention=False):\n",
    "    '''\n",
    "    Tokenize each sentence one at a time and then batch together\n",
    "    '''\n",
    "\n",
    "    # Initialize tokens with the first example\n",
    "    tokens = tokenizer.encode_plus(sentences[0], add_special_tokens=True, max_length=max_length, \n",
    "                                   padding='max_length', truncation=True, \n",
    "                                   return_tensors='tf')\n",
    "    \n",
    "    # Initialize entity mask with first example\n",
    "    if entity_mask == True:\n",
    "        left_mask = [0]*max_length\n",
    "        right_mask = [0]*max_length\n",
    "\n",
    "        # First: Index for [L] entity [L]\n",
    "        idx = np.where(tokens.input_ids[0] == left_token)[0]\n",
    "        for i in range(idx[0]+1, idx[1]):\n",
    "            left_mask[i] = 1\n",
    "        left_mask = tf.constant(left_mask, shape=(1, max_length), dtype='int32')\n",
    "        tokens['left_mask'] = left_mask\n",
    "\n",
    "        # First: Index for [R] entity [R]\n",
    "        idx = np.where(tokens.input_ids[0] == right_token)[0]\n",
    "        for i in range(idx[0]+1, idx[1]):\n",
    "            right_mask[i] = 1\n",
    "        right_mask = tf.constant(right_mask, shape=(1, max_length), dtype='int32')\n",
    "        tokens['right_mask'] = right_mask\n",
    "        \n",
    "    # Initialize global attention mask with first example\n",
    "    if global_attention == True:\n",
    "        global_attention_mask = [0]*max_length\n",
    "        \n",
    "        # Find positions of tokens\n",
    "        left_idx = np.where(tokens.input_ids[0] == left_token)[0]\n",
    "        right_idx = np.where(tokens.input_ids[0] == right_token)[0]\n",
    "        \n",
    "        # Left and Right\n",
    "        for i in range(left_idx[0]+1, left_idx[1]):\n",
    "            global_attention_mask[i] = 1\n",
    "         \n",
    "        for i in range(right_idx[0]+1, right_idx[1]):\n",
    "            global_attention_mask[i] = 1\n",
    "            \n",
    "        # Convert to tf.tensor  \n",
    "        global_attention_mask = tf.constant(global_attention_mask, shape=(1, max_length), dtype='int32')\n",
    "        tokens['global_attention_mask'] = global_attention_mask\n",
    "\n",
    "    # Rest of the examples\n",
    "    for s, sentence in tqdm(enumerate(sentences[1:])):\n",
    "        inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=max_length,\n",
    "                                       padding='max_length', truncation=True, \n",
    "                                       return_tensors='tf')\n",
    "        \n",
    "        tokens.input_ids = tf.concat([tokens.input_ids, inputs.input_ids], 0)\n",
    "        tokens.attention_mask = tf.concat([tokens.attention_mask, inputs.attention_mask], 0)\n",
    "\n",
    "        # Rest of entity mask\n",
    "        if entity_mask == True:\n",
    "            left_mask = [0]*max_length\n",
    "            right_mask = [0]*max_length\n",
    "\n",
    "            # Later: Index for [L] entity [L]\n",
    "            idx = np.where(tokens.input_ids[s+1] == left_token)[0]\n",
    "            for i in range(idx[0]+1, idx[1]):\n",
    "                left_mask[i] = 1\n",
    "            left_mask = tf.constant(left_mask, shape=(1, max_length), dtype='int32')\n",
    "            tokens['left_mask'] = tf.concat([tokens.left_mask, left_mask], 0)  \n",
    "\n",
    "            # Later: Index for [R] entity [R]\n",
    "            idx = np.where(tokens.input_ids[s+1] == right_token)[0]\n",
    "            for i in range(idx[0]+1, idx[1]):\n",
    "                right_mask[i] = 1\n",
    "            right_mask = tf.constant(right_mask, shape=(1, max_length), dtype='int32')\n",
    "            tokens['right_mask'] = tf.concat([tokens.right_mask, right_mask], 0) \n",
    "        \n",
    "        # Rest of global attentino mask\n",
    "        if global_attention == True:\n",
    "            global_attention_mask = [0]*max_length\n",
    "            \n",
    "            # Later: left and right positions\n",
    "            left_idx = np.where(tokens.input_ids[s+1] == left_token)[0]\n",
    "            right_idx = np.where(tokens.input_ids[s+1] == right_token)[0]\n",
    "            \n",
    "            # Left and Right\n",
    "            for i in range(left_idx[0]+1, left_idx[1]):\n",
    "                global_attention_mask[i] = 1\n",
    "\n",
    "            for i in range(right_idx[0]+1, right_idx[1]):\n",
    "                global_attention_mask[i] = 1\n",
    "                \n",
    "            global_attention_mask = tf.constant(global_attention_mask, shape=(1, max_length), dtype='int32')\n",
    "            tokens['global_attention_mask'] = tf.concat([tokens.global_attention_mask, global_attention_mask], 0) \n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fifth-marine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28996 28997\n"
     ]
    }
   ],
   "source": [
    "# Add special tokens\n",
    "extra_tokens = ['[L]', '[R]']\n",
    "tokenizer.add_tokens(extra_tokens, special_tokens=True)\n",
    "\n",
    "left_token = tokenizer.convert_tokens_to_ids('[L]')\n",
    "right_token = tokenizer.convert_tokens_to_ids('[R]')\n",
    "\n",
    "# Resize model vocabulary\n",
    "transformer_model.resize_token_embeddings(len(tokenizer)) \n",
    "\n",
    "print(left_token, right_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "latest-captain",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8999it [00:35, 250.94it/s]\n",
      "11199it [00:57, 193.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input ids\n",
      "tf.Tensor(\n",
      "[  101  1748 10540   102  1387   118  4775   118  5004   102 10296  2236\n",
      "   131 28997  1387   118  4775   118  5004 28997 12398  2236   131  1387\n",
      "   118  4775   118  4925  1555   131 24928 11955  6385 10805  1607  1104\n",
      "  1675  6946   131  1142  1110   170  4376   118  1214   118  1385  1299\n",
      "  1443  1251  2418  1763  2657  1607  1150  2756  1106  1103  5241  1395\n",
      "  1170  1103  4962 15415  1104 16320  1229  1119  1108  5569   170 11639\n",
      "   119  1119  6858  1142 16320  1112  1103  4997 16320  1104  1117  1297\n",
      "  1105  1125  2628 22882  1105 26979  1158  1551  1141   119  1119  1145\n",
      " 10790  1104  4267 15284  1757  1120  1103  1159  1104 16320 15415   119\n",
      "  1119  2756  1106  1103 14609  1324  1185  7719  1162   187  8032  1468\n",
      "  6768   191  1161  2704  5241  1395   117  1187  1126  3288  1246   172\n",
      "  1204 14884  1108  2373  1112  4366   119  1649   117   170   181 25509\n",
      "  1197 23609 26405  5332  1108  1694   117  1107  1134  7159   122  2799\n",
      " 18478  1568  1894  1892  3652  1105  7159   125  2799 21955  1894  1892\n",
      "  3652   119  1103  5351  1108  1850  1111  1126   182  2047  1104  1103\n",
      "  1246   117  1134  2120  1103  2304  1104   170  1268 16530 21831 18593\n",
      "  1126  8816  6834  1306   119  1103   172  1204 14884  1108  3090  1105\n",
      "   117  1113  1248  3622   117  1122  1108  1354  1115  1122  1108 10108\n",
      "  1111   170  4841  4626  1732  2728  2386 23123  1766  1197 19911  1107\n",
      "  1103 28117 20488 11510  5815   172 11419  1179   119  1103  5351  1108\n",
      "  1173  4120  1111 28996  1748 10540 28996   119  2704  1736   131  1103\n",
      "  5351  1108  4120  1106  1103 13467 12885  1920  2587   117  1187   170\n",
      "  1598 21831  1126 10712 12139  1108  1694  1115  1108  7588  2999   117\n",
      "  1443  2554  1104  1251  5119  1126  8816  6834  1306   119  1103  5351\n",
      "  1108  4379  1111  1160  1552  1114   170  9488   172  1204 14884  4000\n",
      "  1185  2607  1121  1103  2166  8179   119  1103 16320 19428 15892  1170\n",
      "  1103  5351  1108  3175  1106  1103  1837   119  1103  5351  1598  1106\n",
      "  1202  1218  1105  1103  2383  1108  1189  1106 12398  1140   119   102\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0], shape=(512,), dtype=int32)\n",
      "\n",
      "attention mask\n",
      "tf.Tensor(\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(512,), dtype=int32)\n",
      "\n",
      "left mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'left_mask'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-1ec40f8fa612>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'left mask'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'right mask'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Batch tokenization - 1000 examples (it) take 6 seconds \n",
    "max_length=512\n",
    "global_attention=False\n",
    "entity_mask=False\n",
    "# X_train = tokenize(max_length, train_df['Text'], \n",
    "#                   tokenizer,\n",
    "#                   left_token=left_token,\n",
    "#                   right_token=right_token,\n",
    "#                   entity_mask=True,\n",
    "#                   global_attention=False)\n",
    "# X_test = tokenize(max_length, test_df['Text'], \n",
    "#                   tokenizer,\n",
    "#                   left_token=left_token,\n",
    "#                   right_token=right_token,\n",
    "#                   entity_mask=True,\n",
    "#                   global_attention=False)\n",
    "X_train = tokenize(max_length, resample_df['Target_Text_Pair'], \n",
    "                  tokenizer,\n",
    "                  left_token=left_token,\n",
    "                  right_token=right_token,\n",
    "                  entity_mask=entity_mask,\n",
    "                  global_attention=global_attention)\n",
    "X_test = tokenize(max_length, test_df['Target_Text_Pair'], \n",
    "                  tokenizer,\n",
    "                  left_token=left_token,\n",
    "                  right_token=right_token,\n",
    "                  entity_mask=entity_mask,\n",
    "                  global_attention=global_attention)\n",
    "\n",
    "print('input ids')\n",
    "print(X_train.input_ids[0])\n",
    "print()\n",
    "if entity_mask==True:\n",
    "    print('attention mask')\n",
    "    print(X_train.attention_mask[0])\n",
    "    print()\n",
    "    print('left mask')\n",
    "    print(X_train.left_mask[0])\n",
    "    print()\n",
    "    print('right mask')\n",
    "    print(X_train.right_mask[0])\n",
    "    print()\n",
    "if global_attention==True:\n",
    "    print('global attention mask')\n",
    "    print(X_train.global_attention_mask[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-bahamas",
   "metadata": {},
   "source": [
    "# Save numpy after tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "sophisticated-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('X_train.input_ids', X_train.input_ids)\n",
    "# np.save('X_train.attention_mask', X_train.attention_mask)\n",
    "# np.save('X_train.left_mask', X_train.left_mask)\n",
    "# np.save('X_train.right_mask', X_train.right_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-niger",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "corresponding-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout,\\\n",
    "                                    Concatenate, GlobalAveragePooling1D, \\\n",
    "                                    BatchNormalization\n",
    "\n",
    "def create_model(freeze_layers=True,\n",
    "                 train_size=None,\n",
    "                 batch_size=1,\n",
    "                 epochs=2,\n",
    "                 initial_learning_rate=1e-5, \n",
    "                 final_learning_rate=1e-6, \n",
    "                 global_attention=False):\n",
    "    \n",
    "    # Input Layers\n",
    "    input_ids = Input(shape=(max_length,), name='input_ids', dtype='int32')\n",
    "    attention_mask = Input(shape=(max_length,), name='attention_mask', dtype='int32') \n",
    "#     left_mask = Input(shape=(max_length,), name='left_mask', dtype='float32') \n",
    "#     right_mask = Input(shape=(max_length,), name='right_mask', dtype='float32') \n",
    "    \n",
    "    if global_attention == False:\n",
    "        # Transformer Layer\n",
    "        bert_out = transformer_model(input_ids=input_ids,\n",
    "                                    attention_mask=attention_mask) \n",
    "    \n",
    "    elif global_attention == True:\n",
    "        print('----Performing Global Attention----')\n",
    "        global_attention_mask = Input(shape=(max_length,), name='global_attention_mask', dtype='int32') \n",
    "        \n",
    "        # Transformer Layer\n",
    "        bert_out = transformer_model(input_ids=input_ids,\n",
    "                              attention_mask=attention_mask,\n",
    "                              global_attention_mask=global_attention_mask)\n",
    "\n",
    "    # Get embeddings and CLS token   \n",
    "    X = bert_out[0]\n",
    "    CLS = bert_out[1]\n",
    "        \n",
    "    # Get embeddings of entity pairs only\n",
    "    # Approach one - dot product, only sums across token\n",
    "    # left = tf.expand_dims(left_mask, -1, name='expand_left_mask')\n",
    "    # right = tf.expand_dims(right_mask, -1, name='expand_right_mask')\n",
    "    # left = tf.matmul(left, X, transpose_a=True, name='matmul_left_mask')\n",
    "    # right = tf.matmul(right, X, transpose_a=True, name='matmul_right_mask')\n",
    "    # Approach two - gather using index\n",
    "#     left_idx = tf.squeeze(tf.where(left_mask[0]==1), axis=1)\n",
    "#     right_idx = tf.squeeze(tf.where(right_mask[0]==1), axis=1)\n",
    "#     left = tf.gather(X, left_idx, axis=1)\n",
    "#     right = tf.gather(X, right_idx, axis=1)\n",
    "#     left = GlobalAveragePooling1D()(left)\n",
    "#     right = GlobalAveragePooling1D()(right)\n",
    "\n",
    "#     X = Concatenate()([CLS, left, right])\n",
    "\n",
    "    # Softmax\n",
    "    X = Dropout(0.5)(CLS)\n",
    "    Y = Dense(3, activation='softmax')(X)\n",
    "    \n",
    "    # Instantiate model\n",
    "    if global_attention == False:\n",
    "            model = Model(inputs=[input_ids, attention_mask], \n",
    "                          outputs = Y)\n",
    "            \n",
    "    elif global_attention == True:\n",
    "            model = Model(inputs=[input_ids, attention_mask, left_mask, right_mask, global_attention_mask], \n",
    "                          outputs = Y)\n",
    "\n",
    "    # Training the embeddings\n",
    "    if freeze_layers == True:\n",
    "        for layer in model.layers[:6]:\n",
    "            layer.trainable = False\n",
    "    elif freeze_layers == False:\n",
    "        for layer in model.layers[:6]:\n",
    "            layer.trainable = True\n",
    "\n",
    "#     for idx, layer in enumerate(transformer_model.layers[0].encoder.layer):\n",
    "#         if freeze_layers != None:\n",
    "#             if idx in freeze_layers:\n",
    "#                 layer.trainable = False\n",
    "#             else: \n",
    "#                 layer.trainable = True\n",
    "#         else:\n",
    "#             layer.trainable = True\n",
    "\n",
    "#         print(layer, layer.trainable)\n",
    "            \n",
    "    # Optimizer and Learning Rate Decay\n",
    "    # learning_rate_decay_factor = (final_learning_rate / initial_learning_rate)**(1/epochs)\n",
    "    # steps_per_epoch = int(train_size/batch_size)\n",
    "    \n",
    "    # lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    #     initial_learning_rate=initial_learning_rate,\n",
    "    #     decay_steps=steps_per_epoch,\n",
    "    #     decay_rate=learning_rate_decay_factor)\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=opt, \n",
    "                  loss='sparse_categorical_crossentropy')\n",
    "    \n",
    "    # Print model summary\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-custom",
   "metadata": {},
   "source": [
    "# Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "graduate-operations",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "asian-forth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 108311808   input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 768)          0           tf_bert_model[3][13]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 3)            2307        dropout_40[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 108,314,115\n",
      "Trainable params: 108,314,115\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... \n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "1800/1800 [==============================] - ETA: 0s - loss: 0.8720WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "1800/1800 [==============================] - 330s 178ms/step - loss: 0.8720 - val_loss: 0.5406\n",
      "Epoch 2/20\n",
      "1800/1800 [==============================] - 318s 177ms/step - loss: 0.4738 - val_loss: 0.3479\n",
      "Epoch 3/20\n",
      "1800/1800 [==============================] - 318s 177ms/step - loss: 0.2697 - val_loss: 0.3030\n",
      "Epoch 4/20\n",
      "1800/1800 [==============================] - 318s 177ms/step - loss: 0.1784 - val_loss: 0.2796\n",
      "Epoch 5/20\n",
      "1800/1800 [==============================] - 318s 177ms/step - loss: 0.1179 - val_loss: 0.3165\n",
      "Epoch 6/20\n",
      "1800/1800 [==============================] - 318s 177ms/step - loss: 0.0868 - val_loss: 0.3665\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import History, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# Decreases learning rate as loss plateaus\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.000001, verbose=1, min_delta=1e-5)\n",
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=2,\n",
    "    verbose=2,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True)\n",
    "\n",
    "# Store loss on tensorboard\n",
    "logdir = \"/home/ubuntu/Longformer/logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir, update_freq=20)\n",
    "\n",
    "# Model params:\n",
    "data = X_train\n",
    "labels = y_train\n",
    "# class_weight = resample_class_weights\n",
    "\n",
    "freeze_layers=False\n",
    "epochs = 20\n",
    "train_size = data.input_ids.shape[0]\n",
    "batch_size = 4\n",
    "initial_learning_rate = 1e-05\n",
    "# final_learning_rate = 1e-06\n",
    "\n",
    "# Create model and fit\n",
    "model = create_model(freeze_layers=freeze_layers, \n",
    "                     epochs=epochs,\n",
    "                     train_size=train_size,\n",
    "                     batch_size=batch_size,\n",
    "                     initial_learning_rate=initial_learning_rate)\n",
    "\n",
    "if global_attention == False:\n",
    "    output = model.fit(x=[data.input_ids, data.attention_mask], \n",
    "                      y=labels, \n",
    "                      batch_size=batch_size,\n",
    "#                       class_weight=class_weight,\n",
    "                      epochs=epochs,\n",
    "                      validation_split=0.2,\n",
    "                      callbacks=[tensorboard_callback, rlr, early_stopping_monitor])\n",
    "elif global_attention == True:\n",
    "    output = model.fit(x=[data.input_ids[0:-1], data.attention_mask[0:-1], \n",
    "                          data.left_mask[0:-1], data.right_mask[0:-1], \n",
    "                          data.global_attention_mask[0:-1]], \n",
    "                      y=labels[0:-1], \n",
    "#                       class_weight=class_weight,\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs,\n",
    "                      validation_split=0.2,\n",
    "                      callbacks=[tensorboard_callback, rlr, early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-practice",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "sublime-sensitivity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "\n",
      "(array([0, 1, 2]), array([3048, 3072, 2880]))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.962     0.977     0.969      3000\n",
      "           1      0.937     0.959     0.948      3000\n",
      "           2      0.971     0.932     0.951      3000\n",
      "\n",
      "    accuracy                          0.956      9000\n",
      "   macro avg      0.956     0.956     0.956      9000\n",
      "weighted avg      0.956     0.956     0.956      9000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pred = model.predict(x=[X_train.input_ids, X_train.attention_mask, \n",
    "                        X_train.left_mask, X_train.right_mask],\n",
    "                     batch_size=4)\n",
    "pred_labels = tf.argmax(pred, axis=1)\n",
    "\n",
    "pred_df = pd.DataFrame({'Actual':y_train.ravel(), 'Predicted':pred_labels.numpy().tolist()})\n",
    "print()\n",
    "print(np.unique(pred_df['Predicted'], return_counts=True))\n",
    "print(classification_report(pred_df['Actual'], pred_df['Predicted'], digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "uniform-thong",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "\n",
      "(array([0, 1, 2]), array([1461, 4251, 5488]))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.350     0.455     0.396      1122\n",
      "           1      0.718     0.749     0.733      4078\n",
      "           2      0.857     0.784     0.819      6000\n",
      "\n",
      "    accuracy                          0.738     11200\n",
      "   macro avg      0.642     0.663     0.649     11200\n",
      "weighted avg      0.756     0.738     0.745     11200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pred = model.predict(x=[X_test.input_ids, X_test.attention_mask],\n",
    "                     batch_size=4)\n",
    "pred_labels = tf.argmax(pred, axis=1)\n",
    "\n",
    "pred_df = pd.DataFrame({'Actual':y_test.ravel(), 'Predicted':pred_labels.numpy().tolist()})\n",
    "print()\n",
    "print(np.unique(pred_df['Predicted'], return_counts=True))\n",
    "print(classification_report(pred_df['Actual'], pred_df['Predicted'], digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "republican-adelaide",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "results_df = test_df.copy()\n",
    "results_df['labels'] = y_test.ravel()\n",
    "results_df['bioclinicalbert_labels'] = pred_labels.numpy().tolist()\n",
    "results_df.to_csv('baseline_pred_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "american-interstate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, encoder_layer_call_and_return_conditional_losses, encoder_layer_call_fn, pooler_layer_call_and_return_conditional_losses while saving (showing 5 of 1295). These functions will not be directly callable after loading.\n",
      "/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: longformer_20210726-033317/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: longformer_20210726-033317/assets\n"
     ]
    }
   ],
   "source": [
    "# Save trained model\n",
    "model.save('longformer_768_20210728-044910')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
