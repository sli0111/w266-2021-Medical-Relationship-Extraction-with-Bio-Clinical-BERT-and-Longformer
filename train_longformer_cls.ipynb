{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "broadband-valley",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-shopping",
   "metadata": {},
   "source": [
    "This notebook use the Longformer with the [CLS] + entity-token.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-factory",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "separated-concentrate",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers\n",
    "!pip install tdqm\n",
    "#!pip install tensorflow\n",
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "referenced-longer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import torch\n",
    "# print(torch.__version__)\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-madness",
   "metadata": {},
   "source": [
    "# Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "massive-connection",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at allenai/longformer-base-4096 were not used when initializing TFLongformerModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFLongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFLongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFLongformerModel were initialized from the model checkpoint at allenai/longformer-base-4096.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFLongformerModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LongformerConfig {\n",
       "  \"_name_or_path\": \"allenai/longformer-base-4096\",\n",
       "  \"attention_mode\": \"longformer\",\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"attention_window\": [\n",
       "    32,\n",
       "    64,\n",
       "    128,\n",
       "    256,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512\n",
       "  ],\n",
       "  \"bos_token_id\": 0,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"ignore_attention_mask\": false,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 4098,\n",
       "  \"model_type\": \"longformer\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"output_attentions\": true,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"sep_token_id\": 2,\n",
       "  \"transformers_version\": \"4.8.2\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer and Models\n",
    "from transformers import LongformerConfig, LongformerTokenizerFast, TFLongformerModel\n",
    "\n",
    "tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096')\n",
    "transformer_model = TFLongformerModel.from_pretrained('allenai/longformer-base-4096',\n",
    "                                                      output_attentions=True,\n",
    "                                                      attention_window=[32, 64, 128, 256, 512, 512, \n",
    "                                                                        512, 512, 512, 512, 512, 512])\n",
    "transformer_model.config\n",
    "\n",
    "# from transformers import AutoTokenizer, TFAutoModel\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "# transformer_model = TFAutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", from_pt=True,\n",
    "#                                                output_attentions=True)\n",
    "# transformer_model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-district",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "neural-train",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>TLINK</th>\n",
       "      <th>Target_A</th>\n",
       "      <th>Target_B</th>\n",
       "      <th>Text</th>\n",
       "      <th>End</th>\n",
       "      <th>Words</th>\n",
       "      <th>left_0</th>\n",
       "      <th>left_1</th>\n",
       "      <th>right_0</th>\n",
       "      <th>right_1</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>612_SECTIME1</td>\n",
       "      <td>AFTER</td>\n",
       "      <td>further evaluation</td>\n",
       "      <td>2014-08-07</td>\n",
       "      <td>Admission Date : [R] 2014-08-07 [R] Discharge ...</td>\n",
       "      <td>end</td>\n",
       "      <td>['Admission' 'Date' ':' '[R]' '2014-08-07' '[R...</td>\n",
       "      <td>188</td>\n",
       "      <td>191</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>123_TL12</td>\n",
       "      <td>OVERLAP</td>\n",
       "      <td>NPO</td>\n",
       "      <td>IVF</td>\n",
       "      <td>Admission Date : 2013-11-21 Discharge Date : 2...</td>\n",
       "      <td>end</td>\n",
       "      <td>['Admission' 'Date' ':' '2013-11-21' 'Discharg...</td>\n",
       "      <td>65</td>\n",
       "      <td>67</td>\n",
       "      <td>69</td>\n",
       "      <td>71</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>413_TL22</td>\n",
       "      <td>AFTER</td>\n",
       "      <td>anti-Candida regimen</td>\n",
       "      <td>consultation</td>\n",
       "      <td>ADMISSION DATE : 5-3-93 DISCHARGE DATE : 5-12-...</td>\n",
       "      <td>end</td>\n",
       "      <td>['ADMISSION' 'DATE' ':' '5-3-93' 'DISCHARGE' '...</td>\n",
       "      <td>173</td>\n",
       "      <td>176</td>\n",
       "      <td>81</td>\n",
       "      <td>83</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>492_TL0</td>\n",
       "      <td>AFTER</td>\n",
       "      <td>elective coronary artery bypass grafting</td>\n",
       "      <td>Admission</td>\n",
       "      <td>[R] Admission [R] Date : 2016-02-14 Discharge ...</td>\n",
       "      <td>end</td>\n",
       "      <td>['[R]' 'Admission' '[R]' 'Date' ':' '2016-02-1...</td>\n",
       "      <td>91</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>337_TL19</td>\n",
       "      <td>AFTER</td>\n",
       "      <td>monitoring</td>\n",
       "      <td>referred</td>\n",
       "      <td>Admission Date : 2017-06-12 Discharge Date : 2...</td>\n",
       "      <td>end</td>\n",
       "      <td>['Admission' 'Date' ':' '2017-06-12' 'Discharg...</td>\n",
       "      <td>79</td>\n",
       "      <td>81</td>\n",
       "      <td>74</td>\n",
       "      <td>76</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            ID    TLINK  \\\n",
       "0           0  612_SECTIME1    AFTER   \n",
       "1           1      123_TL12  OVERLAP   \n",
       "2           2      413_TL22    AFTER   \n",
       "3           3       492_TL0    AFTER   \n",
       "4           4      337_TL19    AFTER   \n",
       "\n",
       "                                   Target_A      Target_B  \\\n",
       "0                        further evaluation    2014-08-07   \n",
       "1                                       NPO           IVF   \n",
       "2                      anti-Candida regimen  consultation   \n",
       "3  elective coronary artery bypass grafting     Admission   \n",
       "4                                monitoring      referred   \n",
       "\n",
       "                                                Text  End  \\\n",
       "0  Admission Date : [R] 2014-08-07 [R] Discharge ...  end   \n",
       "1  Admission Date : 2013-11-21 Discharge Date : 2...  end   \n",
       "2  ADMISSION DATE : 5-3-93 DISCHARGE DATE : 5-12-...  end   \n",
       "3  [R] Admission [R] Date : 2016-02-14 Discharge ...  end   \n",
       "4  Admission Date : 2017-06-12 Discharge Date : 2...  end   \n",
       "\n",
       "                                               Words  left_0  left_1  right_0  \\\n",
       "0  ['Admission' 'Date' ':' '[R]' '2014-08-07' '[R...     188     191        3   \n",
       "1  ['Admission' 'Date' ':' '2013-11-21' 'Discharg...      65      67       69   \n",
       "2  ['ADMISSION' 'DATE' ':' '5-3-93' 'DISCHARGE' '...     173     176       81   \n",
       "3  ['[R]' 'Admission' '[R]' 'Date' ':' '2016-02-1...      91      97        0   \n",
       "4  ['Admission' 'Date' ':' '2017-06-12' 'Discharg...      79      81       74   \n",
       "\n",
       "   right_1  distance  \n",
       "0        5       188  \n",
       "1       71         2  \n",
       "2       83        95  \n",
       "3        2        97  \n",
       "4       76         7  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resample shape:  (9000, 13)\n",
      "test shape:  (11200, 13)\n"
     ]
    }
   ],
   "source": [
    "#train_path = '/home/ubuntu/Longformer/train_test_data/mini_df.csv'\n",
    "test_path = '/home/ubuntu/Longformer/train_test_data/mini_test_df.csv'\n",
    "resample_path = '/home/ubuntu/Longformer/train_test_data/resample_df.csv'\n",
    "\n",
    "# Get all the data\n",
    "test_df = pd.read_csv(test_path)\n",
    "resample_df = pd.read_csv(resample_path)\n",
    "\n",
    "\n",
    "display(resample_df.head())\n",
    "\n",
    "print('resample shape: ', resample_df.shape)\n",
    "print('test shape: ', test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aerial-dealer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "[0.33333333 0.33333333 0.33333333]\n",
      "\n",
      "test\n",
      "[0.10017857 0.36410714 0.53571429]\n"
     ]
    }
   ],
   "source": [
    "# Map labels to numbers\n",
    "TLINK_map = {'AFTER':0,\n",
    "             'OVERLAP': 1,\n",
    "             'BEFORE': 2}\n",
    "\n",
    "y_train = np.asarray(resample_df['TLINK'].apply(lambda x: TLINK_map[x])).reshape(-1,1)\n",
    "y_test = np.asarray(test_df['TLINK'].apply(lambda x: TLINK_map[x])).reshape(-1,1)\n",
    "\n",
    "# Distribution\n",
    "print('train')\n",
    "dist = np.unique(y_train, return_counts=True)[1]/sum(np.unique(y_train, return_counts=True)[1])\n",
    "print(dist)\n",
    "\n",
    "print()\n",
    "print('test')\n",
    "dist = np.unique(y_test, return_counts=True)[1]/sum(np.unique(y_test, return_counts=True)[1])\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-jumping",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "violent-cookie",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(max_length, sentences, tokenizer, \n",
    "             left_token=None, right_token=None, entity_mask=False, \n",
    "             global_attention=False):\n",
    "    '''\n",
    "    Tokenize each sentence one at a time and then batch together\n",
    "    '''\n",
    "\n",
    "    # Initialize tokens with the first example\n",
    "    tokens = tokenizer.encode_plus(sentences[0], add_special_tokens=True, max_length=max_length, \n",
    "                                   padding='max_length', truncation=True, \n",
    "                                   return_tensors='tf')\n",
    "    \n",
    "    # Initialize entity mask with first example\n",
    "    if entity_mask == True:\n",
    "        left_mask = [0]*max_length\n",
    "        right_mask = [0]*max_length\n",
    "\n",
    "        # First: Index for [L] entity [L]\n",
    "        idx = np.where(tokens.input_ids[0] == left_token)[0]\n",
    "        for i in range(idx[0]+1, idx[1]):\n",
    "            left_mask[i] = 1\n",
    "        left_mask = tf.constant(left_mask, shape=(1, max_length), dtype='int32')\n",
    "        tokens['left_mask'] = left_mask\n",
    "\n",
    "        # First: Index for [R] entity [R]\n",
    "        idx = np.where(tokens.input_ids[0] == right_token)[0]\n",
    "        for i in range(idx[0]+1, idx[1]):\n",
    "            right_mask[i] = 1\n",
    "        right_mask = tf.constant(right_mask, shape=(1, max_length), dtype='int32')\n",
    "        tokens['right_mask'] = right_mask\n",
    "        \n",
    "    # Initialize global attention mask with first example\n",
    "    if global_attention == True:\n",
    "        global_attention_mask = [0]*max_length\n",
    "        \n",
    "        # Find positions of tokens\n",
    "        left_idx = np.where(tokens.input_ids[0] == left_token)[0]\n",
    "        right_idx = np.where(tokens.input_ids[0] == right_token)[0]\n",
    "        \n",
    "        # Left and Right\n",
    "        for i in range(left_idx[0]+1, left_idx[1]):\n",
    "            global_attention_mask[i] = 1\n",
    "         \n",
    "        for i in range(right_idx[0]+1, right_idx[1]):\n",
    "            global_attention_mask[i] = 1\n",
    "            \n",
    "        # Convert to tf.tensor  \n",
    "        global_attention_mask = tf.constant(global_attention_mask, shape=(1, max_length), dtype='int32')\n",
    "        tokens['global_attention_mask'] = global_attention_mask\n",
    "\n",
    "    # Rest of the examples\n",
    "    for s, sentence in tqdm(enumerate(sentences[1:])):\n",
    "        inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=max_length,\n",
    "                                       padding='max_length', truncation=True, \n",
    "                                       return_tensors='tf')\n",
    "        \n",
    "        tokens.input_ids = tf.concat([tokens.input_ids, inputs.input_ids], 0)\n",
    "        tokens.attention_mask = tf.concat([tokens.attention_mask, inputs.attention_mask], 0)\n",
    "\n",
    "        # Rest of entity mask\n",
    "        if entity_mask == True:\n",
    "            left_mask = [0]*max_length\n",
    "            right_mask = [0]*max_length\n",
    "\n",
    "            # Later: Index for [L] entity [L]\n",
    "            idx = np.where(tokens.input_ids[s+1] == left_token)[0]\n",
    "            for i in range(idx[0]+1, idx[1]):\n",
    "                left_mask[i] = 1\n",
    "            left_mask = tf.constant(left_mask, shape=(1, max_length), dtype='int32')\n",
    "            tokens['left_mask'] = tf.concat([tokens.left_mask, left_mask], 0)  \n",
    "\n",
    "            # Later: Index for [R] entity [R]\n",
    "            idx = np.where(tokens.input_ids[s+1] == right_token)[0]\n",
    "            for i in range(idx[0]+1, idx[1]):\n",
    "                right_mask[i] = 1\n",
    "            right_mask = tf.constant(right_mask, shape=(1, max_length), dtype='int32')\n",
    "            tokens['right_mask'] = tf.concat([tokens.right_mask, right_mask], 0) \n",
    "        \n",
    "        # Rest of global attentino mask\n",
    "        if global_attention == True:\n",
    "            global_attention_mask = [0]*max_length\n",
    "            \n",
    "            # Later: left and right positions\n",
    "            left_idx = np.where(tokens.input_ids[s+1] == left_token)[0]\n",
    "            right_idx = np.where(tokens.input_ids[s+1] == right_token)[0]\n",
    "            \n",
    "            # Left and Right\n",
    "            for i in range(left_idx[0]+1, left_idx[1]):\n",
    "                global_attention_mask[i] = 1\n",
    "\n",
    "            for i in range(right_idx[0]+1, right_idx[1]):\n",
    "                global_attention_mask[i] = 1\n",
    "                \n",
    "            global_attention_mask = tf.constant(global_attention_mask, shape=(1, max_length), dtype='int32')\n",
    "            tokens['global_attention_mask'] = tf.concat([tokens.global_attention_mask, global_attention_mask], 0) \n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fifth-marine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50265 50266\n"
     ]
    }
   ],
   "source": [
    "# Add special tokens\n",
    "extra_tokens = ['[L]', '[R]']\n",
    "tokenizer.add_tokens(extra_tokens, special_tokens=True)\n",
    "\n",
    "left_token = tokenizer.convert_tokens_to_ids('[L]')\n",
    "right_token = tokenizer.convert_tokens_to_ids('[R]')\n",
    "\n",
    "# Resize model vocabulary\n",
    "transformer_model.resize_token_embeddings(len(tokenizer)) \n",
    "\n",
    "print(left_token, right_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "latest-captain",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8999it [03:27, 43.31it/s] \n",
      "11199it [06:59, 26.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input ids\n",
      "tf.Tensor([    0  9167 12478 ...     1     1     1], shape=(1500,), dtype=int32)\n",
      "\n",
      "attention mask\n",
      "tf.Tensor([1 1 1 ... 0 0 0], shape=(1500,), dtype=int32)\n",
      "\n",
      "left mask\n",
      "tf.Tensor([0 0 0 ... 0 0 0], shape=(1500,), dtype=int32)\n",
      "\n",
      "right mask\n",
      "tf.Tensor([0 0 0 ... 0 0 0], shape=(1500,), dtype=int32)\n",
      "\n",
      "global attention mask\n",
      "tf.Tensor([0 0 0 ... 0 0 0], shape=(1500,), dtype=int32)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Batch tokenization - 1000 examples (it) take 6 seconds \n",
    "max_length=1500\n",
    "global_attention=True\n",
    "# X_train = tokenize(max_length, train_df['Text'], \n",
    "#                   tokenizer,\n",
    "#                   left_token=left_token,\n",
    "#                   right_token=right_token,\n",
    "#                   entity_mask=True,\n",
    "#                   global_attention=False)\n",
    "# X_test = tokenize(max_length, test_df['Text'], \n",
    "#                   tokenizer,\n",
    "#                   left_token=left_token,\n",
    "#                   right_token=right_token,\n",
    "#                   entity_mask=True,\n",
    "#                   global_attention=False)\n",
    "X_train = tokenize(max_length, resample_df['Text'], \n",
    "                  tokenizer,\n",
    "                  left_token=left_token,\n",
    "                  right_token=right_token,\n",
    "                  entity_mask=True,\n",
    "                  global_attention=global_attention)\n",
    "X_test = tokenize(max_length, test_df['Text'], \n",
    "                  tokenizer,\n",
    "                  left_token=left_token,\n",
    "                  right_token=right_token,\n",
    "                  entity_mask=True,\n",
    "                  global_attention=global_attention)\n",
    "\n",
    "print('input ids')\n",
    "print(X_train.input_ids[0])\n",
    "print()\n",
    "print('attention mask')\n",
    "print(X_train.attention_mask[0])\n",
    "print()\n",
    "print('left mask')\n",
    "print(X_train.left_mask[0])\n",
    "print()\n",
    "print('right mask')\n",
    "print(X_train.right_mask[0])\n",
    "print()\n",
    "if global_attention==True:\n",
    "    print('global attention mask')\n",
    "    print(X_train.global_attention_mask[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-bahamas",
   "metadata": {},
   "source": [
    "# Save numpy after tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "sophisticated-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('X_train.input_ids', X_train.input_ids)\n",
    "# np.save('X_train.attention_mask', X_train.attention_mask)\n",
    "# np.save('X_train.left_mask', X_train.left_mask)\n",
    "# np.save('X_train.right_mask', X_train.right_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-niger",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "corresponding-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout,\\\n",
    "                                    Concatenate, GlobalAveragePooling1D, \\\n",
    "                                    BatchNormalization\n",
    "\n",
    "def create_model(freeze_layers=True,\n",
    "                 train_size=None,\n",
    "                 batch_size=1,\n",
    "                 epochs=2,\n",
    "                 initial_learning_rate=1e-5, \n",
    "                 final_learning_rate=1e-6, \n",
    "                 global_attention=False):\n",
    "    \n",
    "    # Input Layers\n",
    "    input_ids = Input(shape=(max_length,), name='input_ids', dtype='int32')\n",
    "    attention_mask = Input(shape=(max_length,), name='attention_mask', dtype='int32') \n",
    "    left_mask = Input(shape=(max_length,), name='left_mask', dtype='float32') \n",
    "    right_mask = Input(shape=(max_length,), name='right_mask', dtype='float32') \n",
    "    \n",
    "    if global_attention == False:\n",
    "        # Transformer Layer\n",
    "        bert_out = transformer_model(input_ids=input_ids,\n",
    "                                    attention_mask=attention_mask) \n",
    "    \n",
    "    elif global_attention == True:\n",
    "        print('----Performing Global Attention----')\n",
    "        global_attention_mask = Input(shape=(max_length,), name='global_attention_mask', dtype='int32') \n",
    "        \n",
    "        # Transformer Layer\n",
    "        bert_out = transformer_model(input_ids=input_ids,\n",
    "                              attention_mask=attention_mask,\n",
    "                              global_attention_mask=global_attention_mask)\n",
    "\n",
    "    # Get embeddings    \n",
    "    X = bert_out[0]\n",
    "    CLS = bert_out[1]\n",
    "        \n",
    "    # Get embeddings of entity pairs only\n",
    "    # Approach one - dot product, only sums across token\n",
    "    # left = tf.expand_dims(left_mask, -1, name='expand_left_mask')\n",
    "    # right = tf.expand_dims(right_mask, -1, name='expand_right_mask')\n",
    "    # left = tf.matmul(left, X, transpose_a=True, name='matmul_left_mask')\n",
    "    # right = tf.matmul(right, X, transpose_a=True, name='matmul_right_mask')\n",
    "    # Approach two - gather using index\n",
    "    left_idx = tf.squeeze(tf.where(left_mask[0]==1), axis=1)\n",
    "    right_idx = tf.squeeze(tf.where(right_mask[0]==1), axis=1)\n",
    "    left = tf.gather(X, left_idx, axis=1)\n",
    "    right = tf.gather(X, right_idx, axis=1)\n",
    "    left = GlobalAveragePooling1D()(left)\n",
    "    right = GlobalAveragePooling1D()(right)\n",
    "\n",
    "    X = Concatenate()([CLS, left, right])\n",
    "\n",
    "    # Softmax\n",
    "    X = Dropout(0.5)(X)\n",
    "    Y = Dense(3, activation='softmax')(X)\n",
    "    \n",
    "    # Instantiate model\n",
    "    if global_attention == False:\n",
    "            model = Model(inputs=[input_ids, attention_mask, left_mask, right_mask], \n",
    "                          outputs = Y)\n",
    "            \n",
    "    elif global_attention == True:\n",
    "            model = Model(inputs=[input_ids, attention_mask, left_mask, right_mask, global_attention_mask], \n",
    "                          outputs = Y)\n",
    "\n",
    "    # Training the embeddings\n",
    "    if freeze_layers == True:\n",
    "        for layer in model.layers[:6]:\n",
    "            layer.trainable = False\n",
    "    elif freeze_layers == False:\n",
    "        for layer in model.layers[:6]:\n",
    "            layer.trainable = True\n",
    "\n",
    "#     for idx, layer in enumerate(transformer_model.layers[0].encoder.layer):\n",
    "#         if freeze_layers != None:\n",
    "#             if idx in freeze_layers:\n",
    "#                 layer.trainable = False\n",
    "#             else: \n",
    "#                 layer.trainable = True\n",
    "#         else:\n",
    "#             layer.trainable = True\n",
    "\n",
    "#         print(layer, layer.trainable)\n",
    "            \n",
    "    # Optimizer and Learning Rate Decay\n",
    "    # learning_rate_decay_factor = (final_learning_rate / initial_learning_rate)**(1/epochs)\n",
    "    # steps_per_epoch = int(train_size/batch_size)\n",
    "    \n",
    "    # lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    #     initial_learning_rate=initial_learning_rate,\n",
    "    #     decay_steps=steps_per_epoch,\n",
    "    #     decay_rate=learning_rate_decay_factor)\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=opt, \n",
    "                  loss='sparse_categorical_crossentropy')\n",
    "    \n",
    "    # Print model summary\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-custom",
   "metadata": {},
   "source": [
    "# Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "graduate-operations",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "asian-forth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Performing Global Attention----\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fa7cae07798>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7fa7ca648f20> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fa7cae07798>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7fa7ca648f20> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "left_mask (InputLayer)          [(None, 1500)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "right_mask (InputLayer)         [(None, 1500)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_2 (Sli (1500,)              0           left_mask[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_3 (Sli (1500,)              0           right_mask[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.eq_2 (TFOpLamb (1500,)              0           tf.__operators__.getitem_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.eq_3 (TFOpLamb (1500,)              0           tf.__operators__.getitem_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "input_ids (InputLayer)          [(None, 1500)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 1500)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "global_attention_mask (InputLay [(None, 1500)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.where_2 (TFOpLambda)         (None, 1)            0           tf.__operators__.eq_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.where_3 (TFOpLambda)         (None, 1)            0           tf.__operators__.eq_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_longformer_model (TFLongform TFLongformerBaseMode 148660992   input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "                                                                 global_attention_mask[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.squeeze_2 (TFOpLam (None,)              0           tf.where_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.squeeze_3 (TFOpLam (None,)              0           tf.where_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.gather_2 (TFOpLamb (None, None, 768)    0           tf_longformer_model[0][13]       \n",
      "                                                                 tf.compat.v1.squeeze_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.gather_3 (TFOpLamb (None, None, 768)    0           tf_longformer_model[0][13]       \n",
      "                                                                 tf.compat.v1.squeeze_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 768)          0           tf.compat.v1.gather_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 768)          0           tf.compat.v1.gather_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 2304)         0           tf_longformer_model[0][14]       \n",
      "                                                                 global_average_pooling1d[0][0]   \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_98 (Dropout)            (None, 2304)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 3)            6915        dropout_98[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 148,667,907\n",
      "Trainable params: 148,667,907\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... \n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "3600/3600 [==============================] - ETA: 0s - loss: 0.8822WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "3600/3600 [==============================] - 2023s 539ms/step - loss: 0.8822 - val_loss: 0.6637\n",
      "Epoch 2/20\n",
      "3600/3600 [==============================] - 1921s 534ms/step - loss: 0.5416 - val_loss: 0.3837\n",
      "Epoch 3/20\n",
      "3600/3600 [==============================] - 1922s 534ms/step - loss: 0.3293 - val_loss: 0.2904\n",
      "Epoch 4/20\n",
      "3600/3600 [==============================] - 1930s 536ms/step - loss: 0.2239 - val_loss: 0.2545\n",
      "Epoch 5/20\n",
      "3600/3600 [==============================] - 1922s 534ms/step - loss: 0.1616 - val_loss: 0.2102\n",
      "Epoch 6/20\n",
      "3600/3600 [==============================] - 1922s 534ms/step - loss: 0.1240 - val_loss: 0.2349\n",
      "Epoch 7/20\n",
      "3600/3600 [==============================] - 1916s 532ms/step - loss: 0.0919 - val_loss: 0.2199\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import History, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# Decreases learning rate as loss plateaus\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.000001, verbose=1, min_delta=1e-5)\n",
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=2,\n",
    "    verbose=2,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True)\n",
    "\n",
    "# Store loss on tensorboard\n",
    "logdir = \"/home/ubuntu/Longformer/logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir, update_freq=20)\n",
    "\n",
    "# Model params:\n",
    "data = X_train\n",
    "labels = y_train\n",
    "# class_weight = resample_class_weights\n",
    "\n",
    "freeze_layers=False\n",
    "epochs = 20\n",
    "train_size = data.input_ids.shape[0]\n",
    "batch_size = 2\n",
    "initial_learning_rate = 1e-05\n",
    "# final_learning_rate = 1e-06\n",
    "\n",
    "# Create model and fit\n",
    "model = create_model(freeze_layers=freeze_layers, \n",
    "                     epochs=epochs,\n",
    "                     train_size=train_size,\n",
    "                     batch_size=batch_size,\n",
    "                     initial_learning_rate=initial_learning_rate,\n",
    "#                      final_learning_rate=final_learning_rate,\n",
    "                     global_attention=global_attention)\n",
    "\n",
    "if global_attention == False:\n",
    "    output = model.fit(x=[data.input_ids, data.attention_mask, \n",
    "                          data.left_mask, data.right_mask], \n",
    "                      y=labels, \n",
    "                      batch_size=batch_size,\n",
    "#                       class_weight=class_weight,\n",
    "                      epochs=epochs,\n",
    "                      validation_split=0.2,\n",
    "                      callbacks=[tensorboard_callback, rlr, early_stopping_monitor])\n",
    "elif global_attention == True:\n",
    "    output = model.fit(x=[data.input_ids[0:-1], data.attention_mask[0:-1], \n",
    "                          data.left_mask[0:-1], data.right_mask[0:-1], \n",
    "                          data.global_attention_mask[0:-1]], \n",
    "                      y=labels[0:-1], \n",
    "#                       class_weight=class_weight,\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs,\n",
    "                      validation_split=0.2,\n",
    "                      callbacks=[tensorboard_callback, rlr, early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-practice",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sublime-sensitivity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "\n",
      "(array([0, 1, 2]), array([3101, 2999, 2900]))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.958     0.991     0.974      3000\n",
      "           1      0.964     0.964     0.964      3000\n",
      "           2      0.988     0.955     0.971      3000\n",
      "\n",
      "    accuracy                          0.970      9000\n",
      "   macro avg      0.970     0.970     0.970      9000\n",
      "weighted avg      0.970     0.970     0.970      9000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pred = model.predict(x=[X_train.input_ids, X_train.attention_mask, \n",
    "                        X_train.left_mask, X_train.right_mask,\n",
    "                        X_train.global_attention_mask],\n",
    "                     batch_size=4)\n",
    "pred_labels = tf.argmax(pred, axis=1)\n",
    "\n",
    "pred_df = pd.DataFrame({'Actual':y_train.ravel(), 'Predicted':pred_labels.numpy().tolist()})\n",
    "print()\n",
    "print(np.unique(pred_df['Predicted'], return_counts=True))\n",
    "print(classification_report(pred_df['Actual'], pred_df['Predicted'], digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "uniform-thong",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(array([0, 1, 2]), array([1719, 4729, 4752]))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.326     0.500     0.395      1122\n",
      "           1      0.662     0.768     0.711      4078\n",
      "           2      0.927     0.734     0.819      6000\n",
      "\n",
      "    accuracy                          0.723     11200\n",
      "   macro avg      0.638     0.667     0.642     11200\n",
      "weighted avg      0.770     0.723     0.737     11200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pred = model.predict(x=[X_test.input_ids, X_test.attention_mask, \n",
    "                        X_test.left_mask, X_test.right_mask,\n",
    "                        X_test.global_attention_mask],\n",
    "                     batch_size=4)\n",
    "pred_labels = tf.argmax(pred, axis=1)\n",
    "\n",
    "pred_df = pd.DataFrame({'Actual':y_test.ravel(), 'Predicted':pred_labels.numpy().tolist()})\n",
    "print()\n",
    "print(np.unique(pred_df['Predicted'], return_counts=True))\n",
    "print(classification_report(pred_df['Actual'], pred_df['Predicted'], digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "republican-adelaide",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "results_df = test_df.copy()\n",
    "results_df['labels'] = y_test.ravel()\n",
    "results_df['longformer_labels'] = pred_labels.numpy().tolist()\n",
    "results_df.to_csv('longformer_1500_window_pred_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "american-interstate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, encoder_layer_call_and_return_conditional_losses, encoder_layer_call_fn, pooler_layer_call_and_return_conditional_losses while saving (showing 5 of 1295). These functions will not be directly callable after loading.\n",
      "/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: longformer_20210726-033317/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: longformer_20210726-033317/assets\n"
     ]
    }
   ],
   "source": [
    "# Save trained model\n",
    "model.save('longformer_768_20210729-034052')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "still-proof",
   "metadata": {},
   "source": [
    "# Prediction on Handcrafted Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "given-slovenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distance between entity pairs\n",
    "def get_distances(results):\n",
    "    results['Token'] = results['Text'].apply(lambda x: tokenizer.encode(x, return_tensors='np'))\n",
    "    results['left_0'] = results['Token'].apply(lambda x: np.where(x[0] == left_token)[0][0])\n",
    "    results['left_1'] = results['Token'].apply(lambda x: np.where(x[0] == left_token)[0][1])\n",
    "    results['right_0'] = results['Token'].apply(lambda x: np.where(x[0] == right_token)[0][0])\n",
    "    results['right_1'] = results['Token'].apply(lambda x: np.where(x[0] == right_token)[0][1])\n",
    "\n",
    "    results = results.drop('Token', axis=1)\n",
    "#     results = results.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "    # Since the order b/w [L] and [R] maybe flipped, use shortest distance\n",
    "    results['left_right'] = abs(results['left_1'] - results['right_0']) \n",
    "    results['right_left'] = abs(results['right_1'] - results['left_0']) \n",
    "\n",
    "    results['distance'] = results[['left_right', 'right_left']].min(axis=1)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "indie-landing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Example</th>\n",
       "      <th>Text</th>\n",
       "      <th>left_0</th>\n",
       "      <th>left_1</th>\n",
       "      <th>right_0</th>\n",
       "      <th>right_1</th>\n",
       "      <th>left_right</th>\n",
       "      <th>right_left</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Admission Date : [R] 2012-03-23 [R] Discharge ...</td>\n",
       "      <td>70</td>\n",
       "      <td>74</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>68</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Admission Date : [L] 2012-03-23 [L] Discharge ...</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>70</td>\n",
       "      <td>74</td>\n",
       "      <td>57</td>\n",
       "      <td>68</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Admission Date : [L] 2012-03-23 [L] Discharge ...</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>171</td>\n",
       "      <td>175</td>\n",
       "      <td>158</td>\n",
       "      <td>169</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Example                                               Text  left_0  left_1  \\\n",
       "0        1  Admission Date : [R] 2012-03-23 [R] Discharge ...      70      74   \n",
       "1        2  Admission Date : [L] 2012-03-23 [L] Discharge ...       6      13   \n",
       "2        3  Admission Date : [L] 2012-03-23 [L] Discharge ...       6      13   \n",
       "\n",
       "   right_0  right_1  left_right  right_left  distance  \n",
       "0        6       13          68          57        57  \n",
       "1       70       74          57          68        57  \n",
       "2      171      175         158         169       158  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example1 = \"Admission Date : [R] 2012-03-23 [R] Discharge Date : 2012-03-26 Service : MEDICINE History of Present \\\n",
    "        Illness : 39 year old male w/ h/o low back pain on chronic narcotics presents after being found \\\n",
    "        [L] unresponsive [L] at home . His daughter awoke him at 7 a.m. , reports he said he felt cold and shivery ,\\\n",
    "        vomited several times , then drove her to school .\"\n",
    "\n",
    "example2 = \"Admission Date : [L] 2012-03-23 [L] Discharge Date : 2012-03-26 Service : MEDICINE History of Present \\\n",
    "        Illness : 39 year old male w/ h/o low back pain on chronic narcotics presents after being found \\\n",
    "        [R] unresponsive [R] at home . His daughter awoke him at 7 a.m. , reports he said he felt cold and shivery ,\\\n",
    "        vomited several times , then drove her to school .\"\n",
    "\n",
    "example3 = \"Admission Date : [L] 2012-03-23 [L] Discharge Date : 2012-03-26 Service : MEDICINE History of Present \\\n",
    "        Illness : 39 year old male w/ h/o low back pain, low back pain, cold and shivery, vomited\\\n",
    "        low back pain, low back pain, cold and shivery, vomited\\\n",
    "        low back pain, low back pain, cold and shivery, vomited\\\n",
    "        low back pain, low back pain, cold and shivery, vomited\\\n",
    "        low back pain,low back pain, cold and shivery, vomited, on chronic narcotics presents after being found \\\n",
    "        [R] unresponsive [R] at home . His daughter awoke him at 7 a.m. , reports he said he felt cold and shivery ,\\\n",
    "        vomited several times , then drove her to school .\"\n",
    "\n",
    "hc_df = pd.DataFrame({'Example': [1, 2, 3], 'Text': [example1, example2, example3]})\n",
    "\n",
    "hc_df = get_distances(hc_df)\n",
    "hc_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "informed-medication",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 346.61it/s]\n"
     ]
    }
   ],
   "source": [
    "X_hc = tokenize(max_length, hc_df['Text'], \n",
    "                  tokenizer,\n",
    "                  left_token=left_token,\n",
    "                  right_token=right_token,\n",
    "                  entity_mask=True,\n",
    "                  global_attention=global_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "cross-punishment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.4002675e-04 6.5801464e-02 9.3405855e-01]\n",
      " [1.7425803e-03 6.9971627e-01 2.9854113e-01]\n",
      " [2.3077088e-03 8.9609867e-01 1.0159367e-01]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int64, numpy=array([2, 1, 1])>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pred = model.predict(x=[X_hc.input_ids, X_hc.attention_mask, \n",
    "                        X_hc.left_mask, X_hc.right_mask,\n",
    "                        X_hc.global_attention_mask],\n",
    "                     batch_size=1)\n",
    "pred_labels = tf.argmax(pred, axis=1)\n",
    "\n",
    "print(pred)\n",
    "pred_labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
